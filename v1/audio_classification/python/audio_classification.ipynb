{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Audio Classification with Google AI Edge LiteRT\n",
        "In this notebook you will use the Google AI Edge LiteRT API to classify audio.\n"
      ],
      "metadata": {
        "id": "k9AyMIIjw23i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6PN9FvIx614"
      },
      "source": [
        "## Preparation\n",
        "The first thing you will need to do is install the necessary dependencies for this sample.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q gradio"
      ],
      "metadata": {
        "id": "HX4RDijCiqTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to download the YAMNet Audio Classification model from [Kaggle models]((https://www.kaggle.com/models/google/yamnet/tfLite)).\n",
        "\n",
        "YAMNet is a deep net that predicts 521 audio event classes from the AudioSet-YouTube corpus it was trained on. It employs the Mobilenet_v1 depthwise-separable convolution architecture."
      ],
      "metadata": {
        "id": "XfOk9gHJikWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.model_download(\"google/yamnet/tfLite/classification-tflite\")\n",
        "print(\"Path to model files:\", path)\n",
        "\n",
        "MODEL_PATH = str(next(pathlib.Path(path).rglob('*.tflite')))"
      ],
      "metadata": {
        "id": "RPw9uq4cJoqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optionally, you can upload your own model (.tflite). If you want to do so, uncomment and run the cell below.\n"
      ],
      "metadata": {
        "id": "-A-NmTcDjp3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# for filename in uploaded:\n",
        "#   content = uploaded[filename]\n",
        "#   with open(filename, 'wb') as f:\n",
        "#     f.write(content)\n",
        "\n",
        "# MODEL_PATH = list(uploaded.keys())[0]\n",
        "\n",
        "# print('Uploaded model:', MODEL_PATH)"
      ],
      "metadata": {
        "id": "T3ev0c4_jrV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install and import libraries"
      ],
      "metadata": {
        "id": "gxcljwPhK_lQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ai-edge-litert-nightly"
      ],
      "metadata": {
        "id": "Ira_HgDZbQ4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4oeBcmdtG5y"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import scipy\n",
        "\n",
        "from ai_edge_litert.interpreter import Interpreter\n",
        "from IPython.display import Audio\n",
        "from scipy.io import wavfile"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the associated files from models\n",
        "The TensorFlow Lite model with metadata and associated files is essentially a zip file that can be unpacked with common zip tools to get the associated files. For example, you can unzip **1.tflite** and extract the labels in the model as follows:\n"
      ],
      "metadata": {
        "id": "bOVa0gdSBsZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_file = zipfile.ZipFile(MODEL_PATH).open('yamnet_label_list.txt')\n",
        "labels = [l.decode('utf-8').strip() for l in labels_file.readlines()]\n",
        "print(len(labels))  # Should print 521"
      ],
      "metadata": {
        "id": "Mte72MX5But-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performing Audio Classification\n",
        "\n",
        "Now that you have the necessary dependencies, it's time to start classifying some audio! While there are a variety of ways to retrieve audio clips, this example will download .wav files of someone whistling and a cat meowing."
      ],
      "metadata": {
        "id": "IuCvSVr_2Afs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the model with the `Interpreter`:"
      ],
      "metadata": {
        "id": "IkBWkAkqi1_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = Interpreter(model_path=MODEL_PATH)"
      ],
      "metadata": {
        "id": "mOPCdkk72FMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next you'll load the input and output details for the model and access `waveform_input_index` and `scores_output_index` using the `index` key"
      ],
      "metadata": {
        "id": "PsyNm1Zn2GTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_details = interpreter.get_input_details()\n",
        "waveform_input_index = input_details[0]['index']\n",
        "output_details = interpreter.get_output_details()\n",
        "scores_output_index = output_details[0]['index']"
      ],
      "metadata": {
        "id": "8kmDqaDg2FPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = interpreter.get_input_details()[0]['shape']\n",
        "input_shape"
      ],
      "metadata": {
        "id": "5c72nUhnWbao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add a method to verify and convert a loaded audio is on the proper sample_rate (16K), otherwise it would affect the model's results."
      ],
      "metadata": {
        "id": "Nt9rtrlcxFhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ensure_sample_rate(original_sample_rate, waveform,\n",
        "                       desired_sample_rate=16000):\n",
        "  \"\"\"Resample waveform if required.\"\"\"\n",
        "  if original_sample_rate != desired_sample_rate:\n",
        "    desired_length = int(round(float(len(waveform)) /\n",
        "                               original_sample_rate * desired_sample_rate))\n",
        "    waveform = scipy.signal.resample(waveform, desired_length)\n",
        "  return desired_sample_rate, waveform"
      ],
      "metadata": {
        "id": "uUHgBn1WxEX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you will download a wav file and listen to it. If you have a file already available, just upload it to colab and use it instead."
      ],
      "metadata": {
        "id": "WHyOa3zSxZ4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://storage.googleapis.com/audioset/speech_whistling2.wav\n",
        "!curl -O https://storage.googleapis.com/audioset/miaow_16k.wav"
      ],
      "metadata": {
        "id": "AyZz3G0SxM4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Choose an audio file\n",
        "wav_file_name = \"speech_whistling2.wav\" # @param [\"miaow_16k.wav\", \"speech_whistling2.wav\"]\n",
        "sample_rate, wav_data = wavfile.read(wav_file_name, 'rb')\n",
        "sample_rate, wav_data = ensure_sample_rate(sample_rate, wav_data)\n",
        "\n",
        "# Show some basic information about the audio.\n",
        "duration = len(wav_data)/sample_rate\n",
        "print(f'Sample rate: {sample_rate} Hz')\n",
        "print(f'Total duration: {duration:.2f}s')\n",
        "print(f'Size of the input: {len(wav_data)}')\n",
        "\n",
        "# Listening to the wav file.\n",
        "Audio(wav_data, rate=sample_rate)"
      ],
      "metadata": {
        "id": "YTqGudoRxXMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The wav_data needs to be normalized to values in [-1.0, 1.0] (as stated in the model's documentation).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mj54qFgDxeL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "waveform = wav_data / tf.int16.max"
      ],
      "metadata": {
        "id": "LMfz-i1dxmbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model you've downloaded has a fixed input window (15600)\n",
        "\n",
        "For a given audio file, you'll have to split it in windows of data of the expected size. The last window might need to be filled with zeros."
      ],
      "metadata": {
        "id": "Hy1w88n6J81G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the audio\n",
        "INPUT_SIZE = 15600\n",
        "splitted_audio_data = tf.signal.frame(waveform, INPUT_SIZE, INPUT_SIZE, pad_end=True, pad_value=0)"
      ],
      "metadata": {
        "id": "Jt_LJ8bEKMM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll loop over all the splitted audio and apply the model for each one of them. Lets also save the result every single time we run the model."
      ],
      "metadata": {
        "id": "cXxk_n2TJufF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for i, data in enumerate(splitted_audio_data):\n",
        "  wavform_data = data.numpy().astype('float32')\n",
        "  # Run the model, check the output.\n",
        "  interpreter.resize_tensor_input(waveform_input_index, [wavform_data.size], strict=True)\n",
        "  interpreter.allocate_tensors()\n",
        "  interpreter.set_tensor(waveform_input_index, wavform_data)\n",
        "  interpreter.invoke()\n",
        "  scores = interpreter.get_tensor(scores_output_index)\n",
        "  results.append(scores)\n",
        "  print(scores.shape)  # Should print (1, 521)\n",
        "  top_class_index = scores.argmax()\n",
        "  infered_class = labels[top_class_index]\n",
        "  print(infered_class)"
      ],
      "metadata": {
        "id": "tfklukm95qUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you'll average the results out to get the final prediction."
      ],
      "metadata": {
        "id": "PB79z5EcKtSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_np = np.array(results)\n",
        "mean_results = results_np.mean(axis=0)\n",
        "result_index = mean_results.argmax()\n",
        "print(f'The main sound is: {labels[result_index]}')"
      ],
      "metadata": {
        "id": "dX-ZRxAUKyeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (Optional) Real-time Audio Classification using Gradio\n",
        "\n",
        "Here you'll rely on Gradio to perform real-time audio classification using the model by gathering all the steps needed to classify audio using the LiteRT API."
      ],
      "metadata": {
        "id": "bRL8UtvpLHm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "\n",
        "def classify(audio):\n",
        "  results = []\n",
        "\n",
        "  # Split the audio\n",
        "  splitted_audio_data = tf.signal.frame(audio, INPUT_SIZE, INPUT_SIZE, pad_end=True, pad_value=0)\n",
        "\n",
        "  for i, data in enumerate(splitted_audio_data):\n",
        "    audio_data = data.numpy().astype('float32')\n",
        "    # Run the model.\n",
        "    interpreter.resize_tensor_input(waveform_input_index, [audio_data.size], strict=True)\n",
        "    interpreter.allocate_tensors()\n",
        "    interpreter.set_tensor(waveform_input_index, audio_data)\n",
        "    interpreter.invoke()\n",
        "    scores = interpreter.get_tensor(scores_output_index)\n",
        "    results.append(scores)\n",
        "\n",
        "  results_np = np.array(results)\n",
        "  mean_results = results_np.mean(axis=0)\n",
        "  result_index = mean_results.argmax()\n",
        "  return f'The main sound is: {labels[result_index]}'\n",
        "\n",
        "\n",
        "def inference(stream, new_chunk):\n",
        "  sample_rate, data = new_chunk\n",
        "  data = data.astype(np.float32)\n",
        "  sample_rate, data = ensure_sample_rate(sample_rate, data)\n",
        "  data /= np.max(np.abs(data))\n",
        "\n",
        "  if stream is not None:\n",
        "    stream = np.concatenate([stream, data])\n",
        "  else:\n",
        "    stream = data\n",
        "\n",
        "  return stream, classify(data)\n",
        "\n",
        "# Gradio parameters\n",
        "title=\"YAMNet\"\n",
        "description=\"An audio event classifier trained on the AudioSet dataset to predict audio events from the AudioSet ontology.\"\n",
        "with gr.Blocks(\n",
        "      title=title,\n",
        "      theme=gr.themes.Soft(primary_hue=gr.themes.colors.blue)\n",
        "  ) as demo:\n",
        "    with gr.Row(equal_height=False):\n",
        "        with gr.Column(scale=5, elem_id=\"audio_classification\"):\n",
        "            gr.Interface(\n",
        "                inference,\n",
        "                [\"state\", gr.Audio(sources=[\"microphone\"], streaming=True)],\n",
        "                [\"state\", \"text\"],\n",
        "                live=True\n",
        "            )\n",
        "\n",
        "demo.queue().launch()"
      ],
      "metadata": {
        "id": "qiEjccA-83H6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mp4-fg1paFXt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}