{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_cQX8dWu4Dv"
      },
      "source": [
        "# Text Classifier with the Google AI Edge LiteRT API\n",
        "\n",
        "This notebook shows you how to use the LiteRT API to classify text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99IjoWCyDk7g"
      },
      "source": [
        "## Preparation\n",
        "\n",
        "Let's start by installing the TensorFlow Model Garden library and Google AI Edge LiteRT library.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q \"tf-models-official==2.16.*\"\n",
        "!pip install ai-edge-litert-nightly"
      ],
      "metadata": {
        "id": "SqcYS1PBv1qC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we download an off-the-shelf model. Check out the [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/text/text_classifier#models) for more text classification models that you can use."
      ],
      "metadata": {
        "id": "7VbnVUy0v0sE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O bert_classifier.tflite -q https://storage.googleapis.com/mediapipe-models/text_classifier/bert_classifier/float32/1/bert_classifier.tflite"
      ],
      "metadata": {
        "id": "SbXbpGZyilMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Choose a text classification model\n",
        "MODEL_PATH = \"bert_classifier.tflite\" # @param [\"bert_classifier.tflite\"]\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eXHX_WoNisxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optionally, you can upload your own model (.tflite). If you want to do so, uncomment and run the cell below.\n"
      ],
      "metadata": {
        "id": "JoVt0vAuxEMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# for filename in uploaded:\n",
        "#   content = uploaded[filename]\n",
        "#   with open(filename, 'wb') as f:\n",
        "#     f.write(content)\n",
        "\n",
        "# MODEL_PATH = list(uploaded.keys())[0]\n",
        "\n",
        "# print('Uploaded model:', MODEL_PATH)"
      ],
      "metadata": {
        "id": "tTODyPWAxBaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the associated files from models\n",
        "The TensorFlow Lite model with metadata and associated files is essentially a zip file that can be unpacked with common zip tools to get the associated files. For example, you can unzip **1.tflite** and extract vocab.txt in the model as follows:\n"
      ],
      "metadata": {
        "id": "mnAR8Vz6tbK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "VOCAB_FILE = 'vocab.txt'\n",
        "\n",
        "with open(VOCAB_FILE, 'wb') as f:\n",
        "  vocab_data = zipfile.ZipFile(MODEL_PATH).open(VOCAB_FILE).read()\n",
        "  f.write(vocab_data)"
      ],
      "metadata": {
        "id": "tLcLdVtHtL0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the model using the Google AI Edge LiteRT API\n",
        "\n",
        "Here you'll be loading the model just so we can save the input shape for future use during tokenization."
      ],
      "metadata": {
        "id": "b85II9TszYvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ai_edge_litert.interpreter import Interpreter\n",
        "\n",
        "# Load the TFLite model and allocate tensors.\n",
        "interpreter = Interpreter(model_path=MODEL_PATH)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Get the input shape\n",
        "input_shape = input_details[0]['shape']\n",
        "input_shape"
      ],
      "metadata": {
        "id": "DkXNQOiszhn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer\n",
        "To run inference using a pre-trained language model such as BERT, you need to make sure that you're using exactly the same tokenization, vocabulary, and index mapping as used during training.\n",
        "\n",
        "The following code rebuilds the tokenizer that was used by the base model using the Model Garden's `tfm.nlp.layers.FastWordpieceBertTokenizer` layer:"
      ],
      "metadata": {
        "id": "h9pPmaIHWnGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_models as tfm\n",
        "\n",
        "do_lower_case = True\n",
        "\n",
        "# Define a tokenizer\n",
        "tokenizer = tfm.nlp.layers.FastWordpieceBertTokenizer(vocab_file=VOCAB_FILE, lower_case=do_lower_case)"
      ],
      "metadata": {
        "id": "DZv7xAsYPjrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's tokenize a test sentence:\n",
        "\n"
      ],
      "metadata": {
        "id": "QnMgvOXPWqV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\"I'm looking forward to it.\"]\n",
        "tokenized_text = tokenizer(text)"
      ],
      "metadata": {
        "id": "eLVPfBK1PuJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pack the inputs\n",
        "\n",
        "TensorFlow Model Garden's `tfm.nlp.layers.BertPackInputs` layer can handle the conversion from a list of tokenized sentences to the input format expected by the BERT model. This layer packs the two input sentences concatenated together. This input is expected to start with a [CLS] \"This is a classification problem\" token, and each sentence should end with a [SEP] \"Separator\" token. It also needs to know the indices of the tokenizer's special tokens."
      ],
      "metadata": {
        "id": "MfHf3XAtWzU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "special = tokenizer.get_special_tokens_dict()\n",
        "special"
      ],
      "metadata": {
        "id": "ArC_AFinXGro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the max seq length matches the model input shape\n",
        "max_seq_length = input_shape[1]\n",
        "\n",
        "packer = tfm.nlp.layers.BertPackInputs(\n",
        "    seq_length=max_seq_length,\n",
        "    special_tokens_dict=tokenizer.get_special_tokens_dict())"
      ],
      "metadata": {
        "id": "PHpBwGZDW-Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try the preprocessing model on some text and see the output:\n",
        "\n"
      ],
      "metadata": {
        "id": "F_cgHBN3xvZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "packed_inputs = packer(tokenized_text)"
      ],
      "metadata": {
        "id": "Ei1FpxITW0TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, it returns a dictionary containing three outputs:\n",
        "\n",
        "* input_word_ids: The tokenized sentences packed together.  \n",
        "* input_mask: The mask indicating which locations are valid in the other\n",
        "outputs.  \n",
        "* input_type_ids: Indicating which sentence each token belongs to."
      ],
      "metadata": {
        "id": "v2hY5-JJ2VqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Keys       : {list(packed_inputs.keys())}')\n",
        "print(f'Shape      : {packed_inputs[\"input_word_ids\"].shape}')\n",
        "print(f'Word Ids   : {packed_inputs[\"input_word_ids\"][0, :12]}')\n",
        "print(f'Input Mask : {packed_inputs[\"input_mask\"][0, :12]}')\n",
        "print(f'Type Ids   : {packed_inputs[\"input_type_ids\"][0, :12]}')"
      ],
      "metadata": {
        "id": "WdxYO-34ghyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Put it all together\n",
        "Combine these two parts into a `keras.layers.Layer` that can be used during inference.\n"
      ],
      "metadata": {
        "id": "dMnDjjJUpNiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class BertInputProcessor(tf.keras.layers.Layer):\n",
        "  def __init__(self, tokenizer, packer):\n",
        "    super().__init__()\n",
        "    self.tokenizer = tokenizer\n",
        "    self.packer = packer\n",
        "\n",
        "  def call(self, inputs):\n",
        "    tokenized = self.tokenizer(inputs)\n",
        "    packed = self.packer(tokenized)\n",
        "    return packed"
      ],
      "metadata": {
        "id": "vgVzA1OJpP0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running inference\n",
        "\n",
        "Here are the steps to run text classification using the LiteRT API."
      ],
      "metadata": {
        "id": "3badT-TlxXOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_TEXT = \"It's been a great day\" # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "9OUV98pqqOOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Import the necessary modules.\n",
        "import numpy as np\n",
        "\n",
        "# STEP 2: Create a text preprocessor.\n",
        "bert_inputs_processor = BertInputProcessor(tokenizer, packer)\n",
        "\n",
        "# STEP 3: Set the input tensors and perform text classification on the input.\n",
        "packed_inputs = bert_inputs_processor(tf.constant([INPUT_TEXT]))\n",
        "interpreter.set_tensor(input_details[0][\"index\"], packed_inputs['input_word_ids'].numpy())\n",
        "interpreter.set_tensor(input_details[1][\"index\"], packed_inputs['input_type_ids'].numpy())\n",
        "interpreter.set_tensor(input_details[2][\"index\"], packed_inputs['input_mask'].numpy())\n",
        "interpreter.invoke()\n",
        "\n",
        "# # STEP 4: Process the classification result. In this case, print out the most likely category.\n",
        "output_tensor = interpreter.get_tensor(output_details[0]['index'])\n",
        "classification_probability = np.argmax(output_tensor)\n",
        "print('Positive' if classification_probability == 1 else 'Negative')"
      ],
      "metadata": {
        "id": "nKxO7mYnQIg1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}