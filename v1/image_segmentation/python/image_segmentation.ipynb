{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_cQX8dWu4Dv"
      },
      "source": [
        "# Image Segmentation using the Google AI Edge LiteRT API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6PN9FvIx614"
      },
      "source": [
        "## Preparation\n",
        "\n",
        "Let's start by importing TensorFlow, LiteRT, and also downloading an off-the-shelf model. Check out [Kaggle Models](https://www.kaggle.com/models/tensorflow/deeplabv3) for more information about the DeepLab V3 that you will be using in this tutorial.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ai-edge-litert-nightly"
      ],
      "metadata": {
        "id": "axIwmHyAjnSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from ai_edge_litert.interpreter import Interpreter\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "fqfklqMcctLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a49D7h4TVmru"
      },
      "source": [
        "## Download the image segmenter model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHXsuWDxpOj4"
      },
      "source": [
        "The next thing you will need to do is download the image segmentation model that will be used for this demo. In this case you will use the DeepLab V3 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMjuVQiDYJKF"
      },
      "outputs": [],
      "source": [
        "#@title Start downloading here.\n",
        "import pathlib\n",
        "import kagglehub\n",
        "\n",
        "path = kagglehub.model_download(\"tensorflow/deeplabv3/tfLite/default\")\n",
        "print(\"Path to model files:\", path)\n",
        "\n",
        "MODEL_PATH = str(next(pathlib.Path(path).rglob('*.tflite')))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optionally, you can upload your own model (.tflite). If you want to do so, uncomment and run the cell below.\n"
      ],
      "metadata": {
        "id": "_hq15mpVigk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# for filename in uploaded:\n",
        "#   content = uploaded[filename]\n",
        "#   with open(filename, 'wb') as f:\n",
        "#     f.write(content)\n",
        "\n",
        "# MODEL_PATH = list(uploaded.keys())[0]\n",
        "\n",
        "# print('Uploaded model:', MODEL_PATH)"
      ],
      "metadata": {
        "id": "V4ObwLBBiedm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown We implemented some functions to visualize the image segmentation results. <br/> Run the following cell to activate the functions.\n",
        "# The visualization utilities here are mostly taken from the DeepLabV3 Demo Colab notebook\n",
        "# https://colab.research.google.com/github/tensorflow/models/blob/master/research/deeplab/deeplab_demo.ipynb\n",
        "\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def create_pascal_label_colormap():\n",
        "  \"\"\"Creates a label colormap used in PASCAL VOC segmentation benchmark.\n",
        "\n",
        "  Returns:\n",
        "    A Colormap for visualizing segmentation results.\n",
        "  \"\"\"\n",
        "  colormap = np.zeros((256, 3), dtype=int)\n",
        "  ind = np.arange(256, dtype=int)\n",
        "\n",
        "  for shift in reversed(range(8)):\n",
        "    for channel in range(3):\n",
        "      colormap[:, channel] |= ((ind >> channel) & 1) << shift\n",
        "    ind >>= 3\n",
        "\n",
        "  return colormap\n",
        "\n",
        "\n",
        "def label_to_color_image(label):\n",
        "  \"\"\"Adds color defined by the dataset colormap to the label.\n",
        "\n",
        "  Args:\n",
        "    label: A 2D array with integer type, storing the segmentation label.\n",
        "\n",
        "  Returns:\n",
        "    result: A 2D array with floating type. The element of the array\n",
        "      is the color indexed by the corresponding element in the input label\n",
        "      to the PASCAL color map.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If label is not of rank 2 or its value is larger than color\n",
        "      map maximum entry.\n",
        "  \"\"\"\n",
        "  if label.ndim != 2:\n",
        "    raise ValueError('Expect 2-D input label')\n",
        "\n",
        "  colormap = create_pascal_label_colormap()\n",
        "\n",
        "  if np.max(label) >= len(colormap):\n",
        "    raise ValueError('label value too large.')\n",
        "\n",
        "  return colormap[label]\n",
        "\n",
        "\n",
        "def visualize_segmentation(image, seg_map):\n",
        "  \"\"\"Visualizes input image, segmentation map and overlay view.\"\"\"\n",
        "  plt.figure(figsize=(15, 5))\n",
        "  grid_spec = gridspec.GridSpec(1, 4, width_ratios=[6, 6, 6, 1])\n",
        "\n",
        "  plt.subplot(grid_spec[0])\n",
        "  plt.imshow(image)\n",
        "  plt.axis('off')\n",
        "  plt.title('Input Image')\n",
        "\n",
        "  plt.subplot(grid_spec[1])\n",
        "  seg_image = label_to_color_image(seg_map).astype(np.uint8)\n",
        "  plt.imshow(seg_image)\n",
        "  plt.axis('off')\n",
        "  plt.title('Segmentation Map')\n",
        "\n",
        "  plt.subplot(grid_spec[2])\n",
        "  plt.imshow(image)\n",
        "  plt.imshow(seg_image, alpha=0.7)\n",
        "  plt.axis('off')\n",
        "  plt.title('Segmentation Overlay')\n",
        "\n",
        "  unique_labels = np.unique(seg_map)\n",
        "  ax = plt.subplot(grid_spec[3])\n",
        "  plt.imshow(\n",
        "      FULL_COLOR_MAP[unique_labels].astype(np.uint8), interpolation='nearest')\n",
        "  ax.yaxis.tick_right()\n",
        "  plt.yticks(range(len(unique_labels)), LABEL_NAMES[unique_labels])\n",
        "  plt.xticks([], [])\n",
        "  ax.tick_params(width=0.0)\n",
        "  plt.grid('off')\n",
        "  plt.show()\n",
        "\n",
        "# Labels for the PASCAL VOC dataset\n",
        "LABEL_NAMES = np.asarray([\n",
        "    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
        "    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
        "    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tv'\n",
        "])\n",
        "\n",
        "FULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1)\n",
        "FULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP)"
      ],
      "metadata": {
        "id": "OhXyZUFwdv_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83PEJNp9yPBU"
      },
      "source": [
        "## Download a test image\n",
        "\n",
        "After downloading the model, it's time to grab an image that you can use for testing! It's worth noting that while this is working with a single image, you can download a collection of images to store in the `IMAGE_FILENAMES` array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzXuqyIBlXer"
      },
      "outputs": [],
      "source": [
        "import urllib\n",
        "\n",
        "IMAGE_FILENAMES = ['segmentation_input_rotation0.jpg']\n",
        "\n",
        "for name in IMAGE_FILENAMES:\n",
        "  url = f'https://storage.googleapis.com/mediapipe-assets/{name}'\n",
        "  urllib.request.urlretrieve(url, name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optionally, you can upload your own image. If you want to do so, uncomment and run the cell below.\n"
      ],
      "metadata": {
        "id": "Tejg2e1aiXoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# for filename in uploaded:\n",
        "#   content = uploaded[filename]\n",
        "#   with open(filename, 'wb') as f:\n",
        "#     f.write(content)\n",
        "# IMAGE_FILENAMES = list(uploaded.keys())\n",
        "\n",
        "# print('Uploaded files:', IMAGE_FILENAMES)"
      ],
      "metadata": {
        "id": "qmys3yONiWY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8XRmapjySMN"
      },
      "source": [
        "## Preview the downloaded image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu_q-Z03r-ed"
      },
      "source": [
        "With the test image downloaded, go ahead and display it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rjHk72-lmHX"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import math\n",
        "\n",
        "# Height and width that will be used by the model\n",
        "DESIRED_HEIGHT = 480\n",
        "DESIRED_WIDTH = 480\n",
        "\n",
        "# Performs resizing and showing the image\n",
        "def resize_and_show(image):\n",
        "  h, w = image.shape[:2]\n",
        "  if h < w:\n",
        "    img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))\n",
        "  else:\n",
        "    img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))\n",
        "  cv2_imshow(img)\n",
        "\n",
        "\n",
        "# Preview the image(s)\n",
        "images = {name: cv2.imread(name) for name in IMAGE_FILENAMES}\n",
        "for name, image in images.items():\n",
        "  print(name)\n",
        "  resize_and_show(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy4r2_ePylIa"
      },
      "source": [
        "## Running inference and visualizing the results\n",
        "To run inference using the Interpreter API, you will need to need the load the model, get the model's input details to get the desired input size and we finally perform image segmentation by running our input image on the model. This example will separate the background and foreground of the image and apply separate colors for them to highlight where each distinctive area exists.\n",
        "\n",
        "Check out the [Interpreter documentation](https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_python) to learn more about configuration options for the Interpreter API.\n",
        "\n",
        "Note: You need to match input/output tensor shapes if you happen to be using custom models."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Import the necessary modules.\n",
        "from PIL import Image\n",
        "from PIL import ImageOps\n",
        "\n",
        "\n",
        "# STEP 2: Load the TFLite model in LiteRT Interpreter and allocate tensors.\n",
        "interpreter = Interpreter(model_path=MODEL_PATH)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# STEP 3: Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# STEP 4: Get image size - converting from BHWC to WH.\n",
        "input_size = input_details[0]['shape'][2], input_details[0]['shape'][1]\n",
        "\n",
        "# Loop through demo image(s)\n",
        "for image_file_name in IMAGE_FILENAMES:\n",
        "  # STEP 5: Load the input image.\n",
        "  image = Image.open(image_file_name)\n",
        "\n",
        "  # STEP 6: Crop to the desired model input size size while keeping the aspect ratio.\n",
        "  cropped_image = ImageOps.contain(image, input_size)\n",
        "\n",
        "  # Step 7: Resize the cropped image to the desired model size\n",
        "  resized_image = cropped_image.convert('RGB').resize(input_size, Image.BILINEAR)\n",
        "\n",
        "  # Step 8: Convert to a NumPy array, add a batch dimension, and normalize the image.\n",
        "  image_np = np.asarray(resized_image).astype(np.float32)\n",
        "  image_np = np.expand_dims(image_np, 0)\n",
        "  image_np = image_np / 127.5 - 1\n",
        "\n",
        "  # Step 9: Set the input tensor and perform segmentation on the input image.\n",
        "  interpreter.set_tensor(input_details[0]['index'], image_np)\n",
        "  interpreter.invoke()\n",
        "  output_tensor = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "  # Step 10: Process the segmentation result. In this case, we visualize it.\n",
        "  width, height = cropped_image.size\n",
        "  segmentation_map = tf.argmax(tf.image.resize(output_tensor, (height, width)), axis=3)\n",
        "  segmentation_map = tf.squeeze(segmentation_map).numpy().astype(np.int8)\n",
        "  visualize_segmentation(cropped_image, segmentation_map)"
      ],
      "metadata": {
        "id": "Y4chj4Rbb-aZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZlksWRCijYYP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}