{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_cQX8dWu4Dv"
      },
      "source": [
        "# Image Classification using the Google AI Edge LiteRT API\n",
        "\n",
        "This notebook shows you how to use the LiteRT Python API to classify images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6PN9FvIx614"
      },
      "source": [
        "## Preparation\n",
        "\n",
        "Let's start with downloading an off-the-shelf model. Check out [Kaggle Models](https://www.kaggle.com/models/tensorflow/efficientnet/tfLite) for more image classification models that you can use.\n",
        "\n",
        "For the different types of model optimization modes used by the models read more [here](https://www.tensorflow.org/lite/performance/post_training_quantization#optimization_methods)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Pick a model\n",
        "effnet_model_type = \"lite0-fp32\" # @param [\"lite0-int8\", \"lite0-fp32\", \"lite0-uint8\"]\n"
      ],
      "metadata": {
        "id": "pjS8KOH4Gc8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.model_download(f\"tensorflow/efficientnet/tfLite/{effnet_model_type}\")\n",
        "print(\"Path to model files:\", path)\n",
        "\n",
        "MODEL_PATH = str(next(pathlib.Path(path).rglob('*.tflite')))"
      ],
      "metadata": {
        "id": "RPw9uq4cJoqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optionally, you can upload your own model (.tflite). If you want to do so, uncomment and run the cell below.\n"
      ],
      "metadata": {
        "id": "2C0fOMZljgw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# for filename in uploaded:\n",
        "#   content = uploaded[filename]\n",
        "#   with open(filename, 'wb') as f:\n",
        "#     f.write(content)\n",
        "\n",
        "# MODEL_PATH = list(uploaded.keys())[0]\n",
        "\n",
        "# print('Uploaded model:', MODEL_PATH)"
      ],
      "metadata": {
        "id": "dtNujN36jdyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89BlskiiyGDC"
      },
      "source": [
        "## Utilities\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4aPO-hvbw3r"
      },
      "outputs": [],
      "source": [
        "#@markdown We implemented some functions to visualize the image classification results. <br/> Run the following cell to activate the functions.\n",
        "from matplotlib import pyplot as plt\n",
        "plt.rcParams.update({\n",
        "    'axes.spines.top': False,\n",
        "    'axes.spines.right': False,\n",
        "    'axes.spines.left': False,\n",
        "    'axes.spines.bottom': False,\n",
        "    'xtick.labelbottom': False,\n",
        "    'xtick.bottom': False,\n",
        "    'ytick.labelleft': False,\n",
        "    'ytick.left': False,\n",
        "    'xtick.labeltop': False,\n",
        "    'xtick.top': False,\n",
        "    'ytick.labelright': False,\n",
        "    'ytick.right': False\n",
        "})\n",
        "\n",
        "\n",
        "def display_one_image(image, title, subplot, titlesize=16):\n",
        "    \"\"\"Displays one image along with the predicted category name and score.\"\"\"\n",
        "    plt.subplot(*subplot)\n",
        "    plt.imshow(image)\n",
        "    if len(title) > 0:\n",
        "        plt.title(title, fontsize=int(titlesize), color='black', fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))\n",
        "    return (subplot[0], subplot[1], subplot[2]+1)\n",
        "\n",
        "def display_batch_of_images(images, predictions):\n",
        "    \"\"\"Displays a batch of images with the classifications.\"\"\"\n",
        "    # Images and predictions.\n",
        "    # Auto-squaring: this will drop data that does not fit into square or square-ish rectangle.\n",
        "    rows = int(math.sqrt(len(images)))\n",
        "    cols = len(images) // rows\n",
        "\n",
        "    # Size and spacing.\n",
        "    FIGSIZE = 13.0\n",
        "    SPACING = 0.1\n",
        "    subplot=(rows,cols, 1)\n",
        "    if rows < cols:\n",
        "        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n",
        "    else:\n",
        "        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n",
        "\n",
        "    # Display.\n",
        "    for i, (image, prediction) in enumerate(zip(images[:rows*cols], predictions[:rows*cols])):\n",
        "        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols) * 40 + 3\n",
        "        subplot = display_one_image(image, prediction, subplot, titlesize=dynamic_titlesize)\n",
        "\n",
        "    # Layout.\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Here we load the labels using the TFLite model metadata\n",
        "import zipfile\n",
        "\n",
        "# Load label list from metadata.\n",
        "try:\n",
        "  with zipfile.ZipFile(MODEL_PATH) as model_with_metadata:\n",
        "    if not model_with_metadata.namelist():\n",
        "      raise ValueError('Invalid TFLite model: no label file found.')\n",
        "\n",
        "    file_name = model_with_metadata.namelist()[0]\n",
        "    with model_with_metadata.open(file_name) as label_file:\n",
        "      label_list = label_file.read().splitlines()\n",
        "      label_list = [label.decode('ascii') for label in label_list]\n",
        "except zipfile.BadZipFile:\n",
        "  print(\n",
        "      'ERROR: Please use models trained with Model Maker or downloaded from TensorFlow Hub.'\n",
        "  )\n",
        "  raise ValueError('Invalid TFLite model: no metadata found.')"
      ],
      "metadata": {
        "id": "WfSq1HUaMR8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83PEJNp9yPBU"
      },
      "source": [
        "## Download test images\n",
        "\n",
        "Let's grab some test images that we'll use later. The images ([1](https://pixabay.com/photos/hamburger-burger-barbeque-bbq-beef-1238246/), [2](https://pixabay.com/photos/cat-kitten-pets-animals-housecat-2934720/)) are from Pixabay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzXuqyIBlXer"
      },
      "outputs": [],
      "source": [
        "import urllib\n",
        "\n",
        "IMAGE_FILENAMES = ['burger.jpg', 'cat.jpg']\n",
        "\n",
        "for name in IMAGE_FILENAMES:\n",
        "  url = f'https://storage.googleapis.com/mediapipe-tasks/image_classifier/{name}'\n",
        "  urllib.request.urlretrieve(url, name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaWcHsQ0lmJP"
      },
      "source": [
        "Optionally, you can upload your own image. If you want to do so, uncomment and run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvdDz3yDlqYR"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# for filename in uploaded:\n",
        "#   content = uploaded[filename]\n",
        "#   with open(filename, 'wb') as f:\n",
        "#     f.write(content)\n",
        "# IMAGE_FILENAMES = list(uploaded.keys())\n",
        "\n",
        "# print('Uploaded files:', IMAGE_FILENAMES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8XRmapjySMN"
      },
      "source": [
        "Then let's check out the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rjHk72-lmHX"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import math\n",
        "\n",
        "DESIRED_HEIGHT = 480\n",
        "DESIRED_WIDTH = 480\n",
        "\n",
        "def resize_and_show(image):\n",
        "  h, w = image.shape[:2]\n",
        "  if h < w:\n",
        "    img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))\n",
        "  else:\n",
        "    img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))\n",
        "  cv2_imshow(img)\n",
        "\n",
        "\n",
        "# Preview the images.\n",
        "\n",
        "images = {name: cv2.imread(name) for name in IMAGE_FILENAMES}\n",
        "for name, image in images.items():\n",
        "  print(name)\n",
        "  resize_and_show(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy4r2_ePylIa"
      },
      "source": [
        "## Running inference and visualizing the results\n",
        "\n",
        "Here are the steps to run image classification using the LiteRT API, as well as load the Google AI Edge LiteRT package.\n",
        "\n",
        "Note: You need to match input/output tensor shapes if you happen to be using custom models."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ai-edge-litert-nightly"
      ],
      "metadata": {
        "id": "bT55xeGdXTvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Import the necessary modules.\n",
        "from ai_edge_litert.interpreter import Interpreter\n",
        "import numpy as np\n",
        "\n",
        "# Default mean and std normalization parameter for float model.\n",
        "MEAN = 127\n",
        "STD = 128\n",
        "\n",
        "# STEP 2: Load the TFLite model into the LiteRT Interpreter and allocate tensors.\n",
        "interpreter = Interpreter(model_path=MODEL_PATH)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# STEP 3: Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# STEP 4: Get the input shape, model input/output quantization flags.\n",
        "_, input_height, input_width, _ = input_details[0]['shape']\n",
        "is_quantized_input = input_details[0]['dtype'] == np.uint8\n",
        "is_quantized_output = output_details[0]['dtype'] == np.uint8\n",
        "\n",
        "\n",
        "images = []\n",
        "predictions = []\n",
        "for image_name in IMAGE_FILENAMES:\n",
        "  # STEP 5: Load the input image.\n",
        "  raw_image = cv2.imread(image_name)\n",
        "  raw_image = cv2.cvtColor(raw_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  # STEP 6: Preprocess the input image as required by the TFLite model.\n",
        "  image = cv2.resize(raw_image, (input_width, input_height))\n",
        "  if not is_quantized_input:\n",
        "    # Normalize the input if it's a float model.\n",
        "    image = (np.float32(image) - MEAN) / STD\n",
        "\n",
        "  # STEP 7: Set the input tensor and classify the input image.\n",
        "  interpreter.set_tensor(input_details[0]['index'], [image])\n",
        "  interpreter.invoke()\n",
        "  output_tensor = np.squeeze(interpreter.get_tensor(output_details[0]['index']))\n",
        "\n",
        "  # STEP 8: Process the classification result. In this case, visualize it.\n",
        "  if is_quantized_output:\n",
        "    # Dequantize the results if the model is quantized.\n",
        "    scale, zero_point = output_details[0]['quantization']\n",
        "    output_tensor = scale * (output_tensor - zero_point)\n",
        "\n",
        "  # Get the top output category index.\n",
        "  top_output_index = np.argmax(output_tensor)\n",
        "\n",
        "  images.append(raw_image)\n",
        "  predictions.append(f\"{label_list[top_output_index]} ({output_tensor[top_output_index]:.2f})\")\n",
        "\n",
        "display_batch_of_images(images, predictions)"
      ],
      "metadata": {
        "id": "IzTzifXA3N4Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}